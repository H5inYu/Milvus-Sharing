{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "HOST = \"milvus\"\n",
    "PORT = \"19530\"\n",
    "COLLECTION_NAME = 'TextSearchCollection'\n",
    "INDEX_PARAMS = {\n",
    "    'metric_type': 'COSINE',\n",
    "    'index_type': 'IVF_FLAT',\n",
    "    'params': {'nlist': 4},\n",
    "}\n",
    "DIMENSTION = 1536 # OpenAI Standard \n",
    " \n",
    "connections.connect(host=HOST, port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name='id', dtype=DataType.INT64, descrition='id', is_primary=True, auto_id=True),\n",
    "        FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=DIMENSTION),\n",
    "        FieldSchema(name='content', dtype=DataType.VARCHAR, max_length=4096)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields)\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    collection.create_index(field_name=\"embedding\", index_params=INDEX_PARAMS)\n",
    "    return collection\n",
    "\n",
    "collection = create_milvus_collection(COLLECTION_NAME, DIMENSTION)\n",
    "collection.load() # load 完之後才可以搜尋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "def handle_text(text: str):\n",
    "    CHUNK_SIZE = 400\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_SIZE//5)\n",
    "\n",
    "    return [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "\n",
    "def parse_content(text):\n",
    "    documents = []\n",
    "    documents = handle_text(text)\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "    return [\n",
    "        {\n",
    "            'embedding': embedding_model.embed_documents([d.page_content])[0],\n",
    "            'content': d.page_content,\n",
    "        } for d in documents\n",
    "    ]\n",
    "\n",
    "with open('./articles/天劍一夢.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    # print(text)\n",
    "    entities = parse_content(text)\n",
    "    print(entities[0])\n",
    "    collection.insert(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ===== Step 1: Embeding ===== #\n",
    "query = '天劍門歷史？'\n",
    "query_embedding = [OpenAIEmbeddings().embed_query(query)]\n",
    "\n",
    "\n",
    "# ===== Step 2: Search knowledge ===== #\n",
    "search_result = collection.search(\n",
    "    data=query_embedding,\n",
    "    anns_field = 'embedding',\n",
    "    limit=5, \n",
    "    param={\n",
    "        'metric_type': 'COSINE', \n",
    "        'params': {},\n",
    "    },\n",
    "    output_fields=['id', 'content'],\n",
    ")\n",
    "\n",
    "for hits in search_result:\n",
    "    for hit in hits:\n",
    "        print(hit.get('id'), hit.get('content'))\n",
    "\n",
    "\n",
    "# ====== Step 3: provide context to LLM ===== #\n",
    "context = '\\n'.join(hit.get('content') for hit in hits for hits in search_result)\n",
    "parser = StrOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template='請根據以下資料回答使用者問題，只能依據資料回答，不得捏造. 問題:{query}. 資料:{context}',\n",
    "    input_variables=['query', 'context'],\n",
    "    \n",
    ")\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "response = chain.invoke({'query': query, 'context': context})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
